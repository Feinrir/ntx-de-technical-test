{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e190ad1-a932-4b36-add7-096b89f98bb2",
   "metadata": {},
   "source": [
    "I started by using Data Analyst GPT environment that is created by ChatGPT team.\n",
    "First i tried uploading the csv directly since it is a publicly available data from GoogleBigQuery and asked chatGPT for a summary but uploading csv directly doesn't seem to work due to security issues and technical constraint. As a workaround chatGPT suggested to paste the data directly or to use external tools like jupyter notebook, since the data I have is too big for chatGPT, i decided to ask chatGPT for data analysis using jupyter notebook.\n",
    "\n",
    "below is the suggested steps by chatGPT with my analysis of the results in bold text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e87099a-a4b2-4b03-ac08-5259d306c0e4",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb338f93-e24f-40da-bb21-2d663f02d9ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries (if not already installed)\n",
    "!pip install pandas matplotlib seaborn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031b8793-1c3c-42f9-87bf-71afcdd69a87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv('ecommerce-session-bigquery.csv') \n",
    "\n",
    "# View the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b98d2f9-c3b4-4951-b49a-b9f9eca0e717",
   "metadata": {},
   "source": [
    "**viewing the first few rows is a basic and good way to start an analysis to check the general state of the data. We can see that the data in the city column have value such as \"(not set)\" and \"not available in demo dataset\" which would need to be cleaned before starting the analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2bd5c5-02f0-410f-8292-c679964fb49e",
   "metadata": {},
   "source": [
    "## 2. Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1090c1-322e-4d2e-b0ed-b3fba5213c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the dataset (rows, columns)\n",
    "print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "\n",
    "# Display column names\n",
    "print(\"Columns:\", df.columns)\n",
    "\n",
    "# Summary of the dataset (data types, null values, etc.)\n",
    "df.info()\n",
    "\n",
    "# Statistical summary for numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e42328-3dca-43b4-bc38-e26a479d4e5e",
   "metadata": {},
   "source": [
    "**another good way to be aware of the data in the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac8ca5f-1bd6-4ea5-8ce1-b996c3b29a5a",
   "metadata": {},
   "source": [
    "## 3. Check for Missing or Duplicated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceca633-2f8c-4588-a055-7d1ab4ebfb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values (if necessary)\n",
    "# df = df.dropna()\n",
    "\n",
    "# Alternatively, fill missing values\n",
    "# df['column_name'] = df['column_name'].fillna(0)  # Replace with appropriate value\n",
    "\n",
    "# Check for duplicate rows\n",
    "print(\"Duplicate Rows:\", df.duplicated().sum())\n",
    "\n",
    "# Drop duplicates if needed\n",
    "# df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d2d5a3-89ab-4a25-9000-369ad74c0b6e",
   "metadata": {},
   "source": [
    "**data quality check for missing or duplicate value, useful but doesn't take into account data that is not null but have weird value since chatGPT doesn't have access to the data (such as the city column value being \"(not set)\" or \"not available in demo dataset\")**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60d9377-5e11-476f-a2d8-92377d624667",
   "metadata": {},
   "source": [
    "Afterwards ChatGPT suggested a few basic analysis but since we already have a case studies, i decided to directly ask ChatGPT on solutions all three problems while copy pasting the top 5 row of data so ChatGPT can get better context, here are the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896b9cb4-2cc3-4e16-85ff-0a17232de4b0",
   "metadata": {},
   "source": [
    "# Problem 1: Identify Top Products Based on the Total Transaction Revenue Per Day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f1e7ba-6d3c-4eda-960e-c7cb7c6c6fd7",
   "metadata": {},
   "source": [
    "## Step 1: Inspect the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf8ddd-0a83-4bc1-b370-c759f17750d5",
   "metadata": {},
   "source": [
    "From the dataset snippet you shared earlier, the key columns we will use are:\n",
    "\n",
    "date: Indicates the day of the session.  \n",
    "v2ProductName: Name of the product.  \n",
    "totalTransactionRevenue: Total revenue for the session (per user).  \n",
    "productRevenue or itemRevenue: Revenue attributed to individual products (if available).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a52c6-13d6-4a08-854b-7c4dc1b15489",
   "metadata": {},
   "source": [
    "**since we already have totalTransactionRevenue, the productRevenue or itemRevenue is redundant so the key column is just going to be date, v2ProductName, and totalTransactionRevenue**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78591e2b-9d84-49e6-893e-0c22df0436a0",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26f18b6-1516-4f44-9f47-23dda3cfbbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')\n",
    "\n",
    "# Ensure that revenue-related columns are numeric\n",
    "df['totalTransactionRevenue'] = pd.to_numeric(df['totalTransactionRevenue'], errors='coerce')\n",
    "df['productRevenue'] = pd.to_numeric(df['productRevenue'], errors='coerce')\n",
    "df['itemRevenue'] = pd.to_numeric(df['itemRevenue'], errors='coerce')\n",
    "\n",
    "# Fill missing revenue values with 0 (if they are NaN)\n",
    "df['totalTransactionRevenue'] = df['totalTransactionRevenue'].fillna(0)\n",
    "df['productRevenue'] = df['productRevenue'].fillna(0)\n",
    "df['itemRevenue'] = df['itemRevenue'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d57145-c2eb-4b39-bd5a-3d512fa557d2",
   "metadata": {},
   "source": [
    "**this is a necessary step since all the value is currently in a string format**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b48fa-9914-4e35-b147-5c8ceac58bee",
   "metadata": {},
   "source": [
    "## Step 3 & 4: Group Data by Product and Date & Identify Top Products Per Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0654e1d5-bc0c-4b86-bb62-cbf1b8687400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'date' and 'v2ProductName', summing up the revenue\n",
    "daily_product_revenue = df.groupby(['date', 'v2ProductName'])['totalTransactionRevenue'].sum().reset_index()\n",
    "\n",
    "# Sort the data by 'date' and 'totalTransactionRevenue' in descending order\n",
    "daily_product_revenue = daily_product_revenue.sort_values(['date', 'totalTransactionRevenue'], ascending=[True, False])\n",
    "\n",
    "# Get the top product per day\n",
    "top_products_per_day = daily_product_revenue.groupby('date').head(1)  # Fetch the top product for each day\n",
    "\n",
    "# Display the results\n",
    "print(top_products_per_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4590fe2b-ea6e-467d-abef-7c464f598b8c",
   "metadata": {},
   "source": [
    "**since this is a relatively simple questions, i think ChatGPT handles it quite well and gives satisfactory results, but the usage of v2ProductName instead of productSKU could cause a grouping mistake. Below i am checking the amount of v2ProductName per productSKU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d5ac6c-a8db-4443-9686-515c9e658fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique v2ProductName per productSKU\n",
    "unique_v2productnames = df.groupby('productSKU')['v2ProductName'].nunique().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "unique_v2productnames.columns = ['productSKU', 'UniqueProductNames']\n",
    "\n",
    "# Sort by the highest number of unique v2ProductName\n",
    "unique_v2productnames = unique_v2productnames.sort_values('UniqueProductNames', ascending=False)\n",
    "\n",
    "# Display the result\n",
    "print(unique_v2productnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48938796-9710-4fc1-ae64-854d75a9acea",
   "metadata": {},
   "source": [
    "**Since we can see some product SKU have multiple product names, the results above might not be accurate. I would redo the same step but using the productSKU instead of v2ProductName for a more accurate results, and then reach out to the client on the proper productSKU to v2ProductName mapping.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c0932f-d1ae-4058-9819-1f3853b0460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'date' and 'productSKU', summing up the revenue\n",
    "daily_product_revenue = df.groupby(['date', 'productSKU'])['totalTransactionRevenue'].sum().reset_index()\n",
    "\n",
    "# Sort the data by 'date' and 'totalTransactionRevenue' in descending order\n",
    "daily_product_revenue = daily_product_revenue.sort_values(['date', 'totalTransactionRevenue'], ascending=[True, False])\n",
    "\n",
    "# Get the top product per day\n",
    "top_products_per_day = daily_product_revenue.groupby('date').head(1)  # Fetch the top product for each day\n",
    "\n",
    "# Display the results\n",
    "print(top_products_per_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef4e054-4c28-4407-9643-d29cac3a0f13",
   "metadata": {},
   "source": [
    "**I would also add how often a product SKU become top transaction revenue where the total transaction revenue is not 0 to shows which product would be beneficial to focus on**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6799ec-4ffe-4be3-9e4f-abb40994b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_products_per_day[top_products_per_day['totalTransactionRevenue'] > 0]['productSKU'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e4d3dd-14ea-4755-805a-6d710e803d5c",
   "metadata": {},
   "source": [
    "# Problem 2: Detect any anomalies, such as a sharp decrease or increase in the number of transactions for a specific product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3deae0-7fb1-4e06-949c-827bf4d6e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate transactions per product over time\n",
    "transactions_per_product = df.groupby(['date', 'productSKU'])['transactions'].sum().reset_index()\n",
    "\n",
    "# Fill missing or NaN transaction values with 0\n",
    "transactions_per_product['transactions'] = transactions_per_product['transactions'].fillna(0)\n",
    "\n",
    "# Sort by date for proper time-series analysis\n",
    "transactions_per_product = transactions_per_product.sort_values(['productSKU', 'date'])\n",
    "\n",
    "# Calculate the daily change in transactions for each product\n",
    "transactions_per_product['daily_change'] = transactions_per_product.groupby('productSKU')['transactions'].diff()\n",
    "\n",
    "# Add a column for the percentage change to better understand magnitude\n",
    "transactions_per_product['percent_change'] = transactions_per_product.groupby('productSKU')['transactions'].pct_change() * 100\n",
    "\n",
    "# Define thresholds for anomalies\n",
    "threshold_percent_change = 50  # e.g., consider changes > 50% as anomalies\n",
    "threshold_transaction_change = 10  # e.g., consider raw changes > 10 as anomalies\n",
    "\n",
    "# Flag anomalies\n",
    "transactions_per_product['anomaly'] = (\n",
    "    (transactions_per_product['daily_change'].abs() > threshold_transaction_change) | \n",
    "    (transactions_per_product['percent_change'].abs() > threshold_percent_change)\n",
    ")\n",
    "\n",
    "# Filter only rows where anomalies are flagged\n",
    "anomalies = transactions_per_product[transactions_per_product['anomaly']]\n",
    "\n",
    "# Display anomalies\n",
    "print(anomalies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6142ad-db63-41f4-8ce5-1fc679984ff3",
   "metadata": {},
   "source": [
    "**The code above is the combination of several step from ChatGPT on how to detect anomalies, while the calculation is not wrong, we can see that the most of the results is -100% of inf, this is because there is not enough transaction per product and one transaction difference is enough to be detected as anomaly. below is a graph showing the total daily transaction by ChatGPT.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8941a5af-dc05-4e31-bcf6-4cf7a5a22368",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_transactions = df.groupby('date')['transactions'].sum().reset_index()\n",
    "\n",
    "# Plot daily transactions as a line graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(daily_transactions['date'], daily_transactions['transactions'], marker='o', linestyle='-', color='blue', label='Daily Transactions')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Daily Transactions Over Time', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Number of Transactions', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9220e0e-365a-4d40-9d19-a5eb6eecf500",
   "metadata": {},
   "source": [
    "**Seeing as the highest total transaction in a day doesn't even reach 30 on later months of 2016 and there's barely any transactions on 2017, i would conclude that we don't have enough data points to conclude anomalies of daily product transactions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eac111-72b5-455c-a500-c986dd30a733",
   "metadata": {},
   "source": [
    "# Problem 3: Identify the most profitable city or province based on the total transaction revenue. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161f7fa-ffc7-43d5-9649-52f6a1805ca0",
   "metadata": {},
   "source": [
    "**Since we don't have province data, I'm going to use city data instead**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd4f872-0dae-4d9c-a87a-3f5289ef4c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by city and sum the total transaction revenue\n",
    "revenue_by_city = df.groupby('city')['totalTransactionRevenue'].sum().reset_index()\n",
    "\n",
    "# Sort the cities by revenue in descending order\n",
    "revenue_by_city = revenue_by_city.sort_values('totalTransactionRevenue', ascending=False)\n",
    "\n",
    "# Display the top 10 most profitable cities\n",
    "print(\"Top 10 Most Profitable Cities by Total Transaction Revenue:\")\n",
    "print(revenue_by_city.head(11))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ede25b-b24a-46bf-b49c-0e5a3e96b80b",
   "metadata": {},
   "source": [
    "**some of the city value is unavailable but I still prefer to include the value to highlight the amount of missing data to the client**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87f429-4a4c-415c-b8a3-788391c1b125",
   "metadata": {},
   "source": [
    "**Alternatively we can show the same data but uses Country data instead since the data is much cleaner**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5821bd9d-8920-4f37-9835-ec3210fda144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by country and sum the total transaction revenue\n",
    "revenue_by_country = df.groupby('country')['totalTransactionRevenue'].sum().reset_index()\n",
    "\n",
    "# Sort the countries by revenue in descending order\n",
    "revenue_by_country = revenue_by_country.sort_values('totalTransactionRevenue', ascending=False)\n",
    "\n",
    "# Display the top 10 most profitable countries\n",
    "print(\"Top 10 Most Profitable Countries by Total Transaction Revenue:\")\n",
    "print(revenue_by_country.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add4b457-fc14-438e-a567-992191688e07",
   "metadata": {},
   "source": [
    "**Since the problem is relatively simple, the results from chatGPT could be used as is**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
